# Hamoye Tag-Along code
This repository showcases the development  of solutions to Machine Learning Quizzes during my participation in the Hamoye Data Science Internship. The work done is explained in stages.

## Stage A 
### Python for Machine Learning
In stage A of my internship at Hamoye, I worked on basic data manipulation. The dataset was provided by the [Food and Agriculture Organization of the United Nations](https://github.com/HamoyeHQ/HDSC-Introduction-to-Python-for-machine-learning), I made use of functions like group by, corr for correlation, sum for summation, mean, max, and std for standard deviation. I also used basic statistical tools to extract information from the dataset.

## Stage B
### Machine Learning: Regression
In this stage, I focus on improving my Machine Learning Regression skills by Predicting the Energy Efficiency of Buildings. The dataset for this stage, which is the Appliances Energy Prediction data, is obtained from the UCI Machine Learning Repository, a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms. The data set is at 10 min for about 4.5 months. I worked on Linear regression analyzing the relationship between the temperature in the living room in Celsius (x = T2) and the temperature outside the building (y = T6). I use linear regression of only one independent variable (T2) to predict a dependent variable(T6). I also worked on Multiple Linear regression where I predicted the output of a dependent variable (Power used by Appliance) based on inputs from multiple independent variables. I used Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) for my evaluation metric.

## Stage C
### Machine Learning: Classification
In this stage, I used Machine Learning Classification algorithms to Predict behaviour to retain customers. You can analyze all relevant customer data and develop focused customer retention programs. The data was obtained from [Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn), The data set includes information about: customers who left within the last month – the column is called Churn. Other information includes services that each customer has signed up for - phone, multiple lines, internet, online security, online backup, device protection, tech support, streaming TV and movies, Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges, and Demographic info about customers – gender, age range, and if they have partners and dependents. I performed initial data preparation by converting the 'TotalCharges' column to numeric values and filling missing values with 0. Then I converted the 'Churn' column to binary values, where 'No' is mapped to 0 and 'Yes' is mapped to 1. Next, I split the data into an 80-20 train-test split with a random state of “1”. I separated the categorical features from the numerical features to perform feature engineering. The numerical features were scaled using StandardScaler and the categorical features were one-hot encoded. Then I used scikit-learn to train a random forest and extra trees classifiers. I also used XGboost and LightGBM to train an extreme boosting model and a light gradient boosting model. I evaluated and compared the models with the Accuracy and F1 Score metrics.

## Stage D
### Neural Network, Image Recognition and Object Detection
In this stage, I built an artificial intelligence algorithm to label satellite image chips with different atmospheric conditions and the different classes of land cover/land use.  For this Multi-class Multi-Label problem, some of the labels are from the following categories: Cloud Cover (clear, partly, cloudy, haze), Primary RainForest, Water (rivers, lakes), Habitation (large city, small homes), Agriculture, Roads etc. The algorithm  will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond. The dataset for this project was obtained from [Kaggle](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/overview).
First I imported the packages I required like pandas, numpy, cv2 for computer vision, Scikit-Learn, and TensorFlow for Keras tools. The model I defined consists of a VGG16 base (without top layers) followed by a dense layer with sigmoid activation, suitable for a binary classification problem where I have 17 classes to predict. I made sure INPUT_SHAPE was defined appropriately before calling create_model() to match my input data dimensions. The VGG16 model encompasses a convolutional neural network architecture up to the fully connected layers preferred for its simplicity and effectiveness in learning hierarchical features from images. I evaluated the model with F1 Beta Metric of 89%.

## Stage E
### Practical Time Series Analysis and Forecast
In the final stage of my internship, I built a time series forecasting model based on measurements from electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. The data was obtained from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption). In this project:
- I plotted the hourly global active power over time with the data resampled to an hourly rate,
- I explored the Pearson correlation between Voltage and multiple features.
- I used Facebook Prophet algorithm to build a Univariate time series model using Global Active Power as the dependent variable and Time with a daily sum sampling rate as the independent variable. I evaluated the model using Mean Absolute Percentage Error (MAPE) and Root Mean Squared Error (RMSE).
- Next, I built a multivariate time series model using Facebook Prophet algorithm. The Global Active Power served as the dependent variable which is Power Consumption and the remaining features were added as regressors to the model over a daily sum sampling rate. I evaluated the model using Mean Absolute Percentage Error (MAPE) and Root Mean Squared Error (RMSE).

