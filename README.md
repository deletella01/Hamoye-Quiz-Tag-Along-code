# Hamoye Tag-Along code
This repository showcases the development  of solutions to Machine Learning Quizzes during my participation in the Hamoye Data Science Internship. The work done is explained in stages.

## Stage A 
In stage A of my internship at Hamoye, I worked on basic data manipulation. The dataset was provided by the Food and Agriculture Organization of the United Nations, I made use of functions like group by, corr for correlation, sum for summation, mean, max, and std for standard deviation. I also used basic statistical tools to extract information from the dataset.

## Stage B
In this stage, I focus on improving my Machine Learning Regression skills by Predicting the Energy Efficiency of Buildings. The dataset for this stage, which is the Appliances Energy Prediction data, is obtained from the UCI Machine Learning Repository, a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms. The data set is at 10 min for about 4.5 months. I worked on Linear regression analyzing the relationship between the temperature in the living room in Celsius (x = T2) and the temperature outside the building (y = T6). I use linear regression of only one independent variable (T2) to predict a dependent variable(T6). I also worked on Multiple Linear regression where I predicted the output of a dependent variable (Power used by Appliance) based on inputs from multiple independent variables. I used Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) for my evaluation metric.

## Stage C
In this stage, I used Machine Learning Classification algorithms to Predict behaviour to retain customers. You can analyze all relevant customer data and develop focused customer retention programs. The data was obtained from Kaggle.com, The data set includes information about: customers who left within the last month – the column is called Churn. Other information includes services that each customer has signed up for - phone, multiple lines, internet, online security, online backup, device protection, tech support, streaming TV and movies, Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges, and Demographic info about customers – gender, age range, and if they have partners and dependents. I performed initial data preparation by converting the 'TotalCharges' column to numeric values and filling missing values with 0. Then I converted the 'Churn' column to binary values, where 'No' is mapped to 0 and 'Yes' is mapped to 1. Next, I split the data into an 80-20 train-test split with a random state of “1”. I separated the categorical features from the numerical features to perform feature engineering. The numerical features were scaled using StandardScaler and the categorical features were one-hot encoded. Then I used scikit-learn to train a random forest and extra trees classifiers. I also used XGboost and LightGBM to train an extreme boosting model and a light gradient boosting model. I evaluated and compared the models with the Accuracy and F1 Score metrics.

## Stage D


## Stage E
